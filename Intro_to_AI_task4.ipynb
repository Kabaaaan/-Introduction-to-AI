{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOidegE7cqffN18cAWpOpCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabaaaan/-Introduction-to-AI/blob/main/Intro_to_AI_task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9L1QbkMgzV_8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Загрузка MNIST датасета...\")\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
        "X = mnist.data.astype(np.float32)\n",
        "y = mnist.target.astype(np.int32)\n",
        "\n",
        "X = X / 255.0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
        "\n",
        "train_dataset = [(X_train[i][None, ...], y_train[i]) for i in range(len(X_train))]\n",
        "test_dataset = [(X_test[i][None, ...], y_test[i]) for i in range(len(X_test))]\n",
        "\n",
        "print(f\"Размер обучающей выборки: {len(train_dataset)}\")\n",
        "print(f\"Размер тестовой выборки: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "ECquLA9nzXtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Гиперпараметры\n",
        "INPUT_DIM = 784\n",
        "OUT_DIM = 10\n",
        "H_DIM = 32\n",
        "\n",
        "# Обучаемые параметры\n",
        "ALPHA = 0.001\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 100"
      ],
      "metadata": {
        "id": "3BIucdPEzbrd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def softmax(t):\n",
        "#     out = np.exp(t)\n",
        "#     return out / np.sum(out)\n",
        "#\n",
        "# def cross_entropy(z, y):\n",
        "#     return -np.log(z[0, y])\n",
        "#\n",
        "# def to_one_hot(y, num_classes):\n",
        "#     y_full = np.zeros((1, num_classes))\n",
        "#     y_full[0, y] = 1\n",
        "#     return y_full\n",
        "\n",
        "def he_init(w):\n",
        "    return w * np.sqrt(2.0 / INPUT_DIM)\n",
        "\n",
        "def relu(t):\n",
        "    return np.maximum(t, 0)\n",
        "\n",
        "def relu_deriv(t):\n",
        "    return (t >= 0).astype(float)\n",
        "\n",
        "def softmax_batch(t):\n",
        "    out = np.exp(t)\n",
        "    return out / np.sum(out, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_batch(z, y):\n",
        "    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))\n",
        "\n",
        "def to_one_hot_batch(y, num_classes):\n",
        "    y_full = np.zeros((len(y), num_classes))\n",
        "    for j, yj in enumerate(y):\n",
        "        y_full[j, yj] = 1\n",
        "    return y_full"
      ],
      "metadata": {
        "id": "m8WvbgNazgkL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUV47GwzfMCb"
      },
      "outputs": [],
      "source": [
        "# Инициализация весов\n",
        "W1 = he_init(np.random.randn(INPUT_DIM, H_DIM))\n",
        "b1 = np.zeros((1, H_DIM))\n",
        "\n",
        "W2 = he_init(np.random.randn(H_DIM, H_DIM))\n",
        "b2 = np.zeros((1, H_DIM))\n",
        "\n",
        "W3 = he_init(np.random.randn(H_DIM, OUT_DIM))\n",
        "b3 = np.zeros((1, OUT_DIM))\n",
        "\n",
        "loss_arr = []\n",
        "\n",
        "for ep in range(NUM_EPOCHS):\n",
        "    random.shuffle(train_dataset)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for i in range(len(train_dataset) // BATCH_SIZE):\n",
        "        batch_x, batch_y = zip(*train_dataset[i*BATCH_SIZE : i*BATCH_SIZE+BATCH_SIZE])\n",
        "        x = np.concatenate(batch_x, axis=0)\n",
        "        y = np.array(batch_y)\n",
        "\n",
        "        # Прямое распространение\n",
        "        t1 = x @ W1 + b1 # матричное умножение\n",
        "        h1 = relu(t1)\n",
        "        t2 = h1 @ W2 + b2\n",
        "        h2 = relu(t2)\n",
        "        t3 = h2 @ W3 + b3\n",
        "        z = softmax_batch(t3)\n",
        "        E = np.sum(cross_entropy_batch(z, y))\n",
        "\n",
        "        # Обратное распространение\n",
        "        y_full = to_one_hot_batch(y, OUT_DIM)\n",
        "\n",
        "        dE_dt3 = z - y_full\n",
        "        dE_dW3 = h2.T @ dE_dt3\n",
        "        dE_db3 = np.sum(dE_dt3, axis=0, keepdims=True)\n",
        "\n",
        "        dE_dh2 = dE_dt3 @ W3.T\n",
        "        dE_dt2 = dE_dh2 * relu_deriv(t2)\n",
        "        dE_dW2 = h1.T @ dE_dt2\n",
        "        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)\n",
        "\n",
        "        dE_dh1 = dE_dt2 @ W2.T\n",
        "        dE_dt1 = dE_dh1 * relu_deriv(t1)\n",
        "        dE_dW1 = x.T @ dE_dt1\n",
        "        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)\n",
        "\n",
        "        # Обновление параметров\n",
        "        W1 = W1 - ALPHA * dE_dW1\n",
        "        b1 = b1 - ALPHA * dE_db1\n",
        "        W2 = W2 - ALPHA * dE_dW2\n",
        "        b2 = b2 - ALPHA * dE_db2\n",
        "        W3 = W3 - ALPHA * dE_dW3\n",
        "        b3 = b3 - ALPHA * dE_db3\n",
        "\n",
        "        total_loss += E\n",
        "        batch_count += 1\n",
        "\n",
        "    avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
        "    loss_arr.append(avg_loss)\n",
        "    print(f\"Эпоха {ep+1}/{NUM_EPOCHS}, Средний loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):\n",
        "    t1 = x @ W1 + b1\n",
        "    h1 = relu(t1)\n",
        "    t2 = h1 @ W2 + b2\n",
        "    h2 = relu(t2)\n",
        "    t3 = h2 @ W3 + b3\n",
        "    z = softmax_batch(t3)\n",
        "    return z\n",
        "\n",
        "def calc_accuracy(dataset):\n",
        "    correct = 0\n",
        "    for x, y in dataset:\n",
        "        z = predict(x)\n",
        "        y_pred = np.argmax(z)\n",
        "        if y_pred == y:\n",
        "            correct += 1\n",
        "    acc = correct / len(dataset)\n",
        "    return acc\n",
        "\n",
        "train_accuracy = calc_accuracy(train_dataset[:1000])\n",
        "test_accuracy = calc_accuracy(test_dataset)\n",
        "\n",
        "print(f\"Точность на обучающей выборке: {train_accuracy:.4f}\")\n",
        "print(f\"Точность на тестовой выборке: {test_accuracy:.4f}\")\n",
        "\n",
        "plt.plot(loss_arr)\n",
        "plt.title('Loss во время обучения')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dl5kUEPdzSpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(dataset, num_examples=12):\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        x, true_label = dataset[i]\n",
        "        z = predict(x)\n",
        "        pred_label = np.argmax(z)\n",
        "        confidence = z[0, pred_label]\n",
        "\n",
        "        axes[i].imshow(x.reshape(28, 28), cmap='gray')\n",
        "        axes[i].set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.3f}')\n",
        "\n",
        "        if pred_label != true_label:\n",
        "            axes[i].title.set_color('red')\n",
        "\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_predictions(test_dataset)"
      ],
      "metadata": {
        "id": "_aBVM1QmzNuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(dataset, title):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for x, y in dataset[:1000]:\n",
        "        z = predict(x)\n",
        "        y_pred.append(np.argmax(z))\n",
        "        y_true.append(y)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    disp.plot(ax=ax, cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(test_dataset, 'Матрица ошибок на тестовой выборке')"
      ],
      "metadata": {
        "id": "d8kCQJLAzJbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}